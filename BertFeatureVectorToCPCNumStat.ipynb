{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wei-yuan/patent_classification/blob/master/BertFeatureVectorToCPCNumStat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project: From The First Claim of Patent to CPC Number Statistics"
      ],
      "metadata": {
        "id": "9bS2L5L1NEsD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi2wglYRcwiA"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "NHcEzO5G4_jR"
      },
      "outputs": [],
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q -U \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "0Z43CgQsZyVQ"
      },
      "outputs": [],
      "source": [
        "# You will use the AdamW optimizer from tensorflow/models.\n",
        "!pip install -q tf-models-official==2.7.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk_3SfSE-7a-"
      },
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "vc4_C8QgcqPx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsy8AzM2_AH_"
      },
      "source": [
        "## Load Pre-Trained Model of BERT from The Following Website\n",
        "\n",
        "* BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
        "* Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBwEarBLijCg",
        "outputId": "782e390b-de45-466f-9a1d-e2d71797704e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ],
      "source": [
        "#@title Choose a BERT model to fine-tune\n",
        "\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB1ZiePUgwQL"
      },
      "source": [
        "The preprocessing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "5KaRQHFf-_rw"
      },
      "outputs": [],
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-uLwfL1_MI-"
      },
      "source": [
        "Load Test Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXXbx6kC_Ohz",
        "outputId": "d4ac5188-f18c-4e64-9c3c-9d36947948e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a method for controlling an electronic apparatus, the method comprising: receiving image data, an artificial intelligence (ai) flag indicating whether ai downscaling is performed by an external server, and a filter index from the external server; decoding the image data; in response to the ai flag being a first value and the filter index being not null, upscaling the decoded image data using a first ai model corresponding to the filter index and providing the upscaled image data for output; in response to the ai flag being the first value and the filter index being null, upscaling the decoded image data using a default ai model and providing the upscaled image data for output; and in response to the ai flag being not the first value, providing the decoded image data for output without performing an upscaling process, wherein the image data is obtained by encoding downscaled image data acquired by inputting original image data corresponding to the image data into a second ai model for downscaling original image data, wherein a number of filters of the first ai model is smaller than a number of filters of the second ai model, and wherein the first ai model is a convolutional neural network (cnn).\n"
          ]
        }
      ],
      "source": [
        "claimString = ('A method for controlling an electronic apparatus, '\n",
        "            + 'the method comprising: receiving image data, '\n",
        "            + \"an artificial intelligence (AI) flag indicating whether AI downscaling is performed by an external server, \"\n",
        "            + \"and a filter index from the external server; decoding the image data; \"\n",
        "            + \"in response to the AI flag being a first value and the filter index being not null, \"\n",
        "            + \"upscaling the decoded image data using a first AI model corresponding to the filter index and providing the upscaled image data for output; \"\n",
        "            + \"in response to the AI flag being the first value and the filter index being null, \" \n",
        "            + \"upscaling the decoded image data using a default AI model and providing the upscaled image data for output; \" \n",
        "            + \"and in response to the AI flag being not the first value, providing the decoded image data for output without performing an upscaling process, \" \n",
        "            + \"wherein the image data is obtained by encoding downscaled image data acquired \" \n",
        "            + \"by inputting original image data corresponding to the image data into a second AI model for downscaling original image data, \" \n",
        "            + \"wherein a number of filters of the first AI model is smaller than a number of filters of the second AI model, \" \n",
        "            + \"and wherein the first AI model is a Convolutional Neural Network (CNN).\")\n",
        "\n",
        "claimString = claimString.lower()\n",
        "print(claimString)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check The Output Of Test Text"
      ],
      "metadata": {
        "id": "hSjqFuX0NX7x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E1kwT0hi5mM",
        "outputId": "9703aa18-d3ad-45c5-a8d5-90dd50e56063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys       : ['input_type_ids', 'input_word_ids', 'input_mask']\n",
            "Shape      : (1, 128)\n",
            "Word Ids   : [  101  1037  4118  2005  9756  2019  4816 14709  1010  1996  4118  9605]\n",
            "Input Mask : [1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "text_test = [claimString]\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVCieTEj_Ozs"
      },
      "source": [
        "Generate Feature Vector from Test Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u2VKISA_PAR"
      },
      "outputs": [],
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ],
      "metadata": {
        "id": "BKIyEtQsZyNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Drive"
      ],
      "metadata": {
        "id": "2aB06_C6bI59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "0V_07x2dbkHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read CSV"
      ],
      "metadata": {
        "id": "HaPlp3u-YmUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.__version__"
      ],
      "metadata": {
        "id": "WR3qCxrzacOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('gdrive/My Drive/out_sourcing/patent_classification/patent_info.csv')     \n",
        "df.head()"
      ],
      "metadata": {
        "id": "zdSCbDikYmq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Feature Vector to column claim1_feature_vector"
      ],
      "metadata": {
        "id": "W39QUQRLyNsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "claim1_series = df[\"claim1\"]\n",
        "claim1_feature_vector_list = list()\n",
        "\n",
        "for index, value in claim1_series.items():\n",
        "    text_test = [value]\n",
        "    text_preprocessed = bert_preprocess_model(text_test)\n",
        "    bert_results = bert_model(text_preprocessed)\n",
        "    claim1_feature_vector_list.append(bert_results[\"pooled_output\"].numpy()[0])  # get only 1d vector "
      ],
      "metadata": {
        "id": "TFUB0Fdsk5zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"the number of data in claim1 feature list: {len(claim1_feature_vector_list)}\")\n",
        "length = 5\n",
        "for index, vector in enumerate(claim1_feature_vector_list, start = 1):\n",
        "    print(f\"the vector size: {vector.shape}\")\n",
        "    print(f\"the {index} claim1 feature vector in only {length} digits: {vector[:length]}\")"
      ],
      "metadata": {
        "id": "PoPOVojCoMbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means Clustering"
      ],
      "metadata": {
        "id": "WK01VuoCzDwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()  # for plot styling\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "HbfUgzEJzHar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "number_of_cluster = 5  # <--- input the number of clusters here\n",
        "kmeans = KMeans(n_clusters = number_of_cluster)\n",
        "kmeans.fit(claim1_feature_vector_list)\n",
        "y_kmeans = kmeans.predict(claim1_feature_vector_list)"
      ],
      "metadata": {
        "id": "k77vq1cQzXFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The class number after K-Means: {y_kmeans}\")"
      ],
      "metadata": {
        "id": "ACu3-rIMzDKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensional Reduction For High Dimension Vector\n",
        "The working flow: vector -> PCA -> tSNE"
      ],
      "metadata": {
        "id": "St2Zg10h-Mfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scprep phate umap-learn"
      ],
      "metadata": {
        "id": "uGTrOAeSAouT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scprep"
      ],
      "metadata": {
        "id": "RGTAUXBXA7DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Speed it up by running PCA to 50 dimensions.\n",
        "data = np.array(claim1_feature_vector_list)\n",
        "print(f\"The data shape: {data.shape}\")\n",
        "\n",
        "# n_components must be between 0 and min(n_samples, n_features)=5\n",
        "n_samples, n_features = data.shape[0], data.shape[1]\n",
        "data_pca = scprep.reduce.pca(data, n_components=min(n_samples, n_features), method='dense')"
      ],
      "metadata": {
        "id": "7oBEXf9g-Lte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speed up t-SNE a little further by subsampling"
      ],
      "metadata": {
        "id": "ge4qq4xkFS8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.manifold\n",
        "tsne_op = sklearn.manifold.TSNE(n_components=2, perplexity=30)\n",
        "data_tsne = tsne_op.fit_transform(data)"
      ],
      "metadata": {
        "id": "1xCBWMtPFf7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scprep.plot.scatter2d(data_tsne, c=y_kmeans,\n",
        "                      figsize=(8,4), legend_anchor=(1,1),\n",
        "                      ticks=False, label_prefix='t-SNE', \n",
        "                      title=\"K-Means by t-SNE\")"
      ],
      "metadata": {
        "id": "aVv_E-ptF2Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update New Series Data To Dataframe"
      ],
      "metadata": {
        "id": "ZDcemxzU3B1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_series = pd.Series(claim1_feature_vector_list)\n",
        "print(f\"claim 1 feature vector: {c_series}\")"
      ],
      "metadata": {
        "id": "IG4f5Q3wn-HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update claim 1 feature vector\n",
        "df[\"claim1_feature_vector\"] = c_series"
      ],
      "metadata": {
        "id": "1pvU-w2QtXBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"claim1_feature_vector\"])"
      ],
      "metadata": {
        "id": "6Ojpggp_wF-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update cluster number\n",
        "df[\"cluster\"] = y_kmeans"
      ],
      "metadata": {
        "id": "K5ucDjnKpJZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "6e6hVQU7v6aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Classification By Cluster Number"
      ],
      "metadata": {
        "id": "UMemg-J8mzwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplify CPC Number"
      ],
      "metadata": {
        "id": "N8CAAEDjrLdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select cluster number\n",
        "cluster_unique = df[\"cluster\"].unique().tolist()\n",
        "cluster_unique.sort()  # sort in the acsending way\n",
        "print(f\"cluster number: {cluster_unique}\")\n",
        "\n",
        "for cluster_num in cluster_unique:\n",
        "    print(f\"cluster number: {cluster_num}\")\n",
        "    single_cluster = df[df[\"cluster\"] == cluster_num]\n",
        "    break"
      ],
      "metadata": {
        "id": "KY6oy3FJrSvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the result of cluster 0\n",
        "single_cluster.head()"
      ],
      "metadata": {
        "id": "uQK83hw5PZwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CPC Number Category\n",
        "* Section: F\n",
        "* Class: F02\n",
        "* Sub-class: F02D\n",
        "* Group: F02D 41\n",
        "* Sub-Group: F02D 41/02"
      ],
      "metadata": {
        "id": "_u9ZZwkS2l1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read each row data in cpc number\n",
        "cpc_number_list = single_cluster[\"cpc number\"].tolist()\n",
        "print(f\"Number of element in cpc_number_list: {len(cpc_number_list)}\")\n",
        "print(f\"The content of cpc_number_list: {cpc_number_list}\")"
      ],
      "metadata": {
        "id": "hTuxmlv2slCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cpc_num_stat(mode: str, cpc_number_list: list, simple_cpc_number_dict: dict):    \n",
        "    for row in cpc_number_list:\n",
        "        # split by semi-colon\n",
        "        row_split_by_semi =  row.split(';')\n",
        "        for row in row_split_by_semi:        \n",
        "            cpc_num_set = set()      \n",
        "            # split by space\n",
        "            row_split = row.split(' ')        \n",
        "            row_split_filtered_empty_string = [x for x in row_split if x]\n",
        "            # to sub-class level\n",
        "            sub_class = row_split_filtered_empty_string[0]      \n",
        "            # # to sub-group level\n",
        "            sub_group = ' '.join(row_split_filtered_empty_string[:2])  # stop until the date information\n",
        "\n",
        "            if MODE == 'SUB-CLASS':\n",
        "                data = sub_class\n",
        "            elif MODE == 'SUB-GROUP':\n",
        "                data = sub_group\n",
        "\n",
        "            # add to simple_cpc_number_dict          \n",
        "            if sub_class not in simple_cpc_number_dict:\n",
        "                simple_cpc_number_dict[data] = 1\n",
        "            else:\n",
        "                simple_cpc_number_dict[data] += 1\n",
        "    return simple_cpc_number_dict"
      ],
      "metadata": {
        "id": "QmYpOZdV7J5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# switch mode here\n",
        "MODE = 'SUB-CLASS'\n",
        "stat_dictionary = dict()\n",
        "result = cpc_num_stat(MODE, cpc_number_list, stat_dictionary)"
      ],
      "metadata": {
        "id": "3zBogov9CetY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "id": "QfbyjE0f669f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# switch mode here\n",
        "MODE = 'SUB-GROUP'\n",
        "stat_dictionary = dict()\n",
        "result = cpc_num_stat(MODE, cpc_number_list, stat_dictionary)"
      ],
      "metadata": {
        "id": "Lkkksm-oDNKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "id": "rbo1VAZoDNVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# iterate all class"
      ],
      "metadata": {
        "id": "98VY2U4MIo92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate each cluster number\n",
        "for cluster_num in cluster_unique:\n",
        "    print(f\"cluster number: {cluster_num}\")\n",
        "    single_cluster = df[df[\"cluster\"] == cluster_num]\n",
        "    cpc_number_list = single_cluster[\"cpc number\"].tolist()\n",
        "    # switch mode here\n",
        "    MODE = 'SUB-CLASS'\n",
        "    stat_dictionary = dict()\n",
        "    result = cpc_num_stat(MODE, cpc_number_list, stat_dictionary) \n",
        "    print(f\"CPC number stats in sub-class level: {result}\")\n",
        "    \n",
        "    result = None\n",
        "    # switch mode here\n",
        "    MODE = 'SUB-GROUP'\n",
        "    stat_dictionary = dict()\n",
        "    result = cpc_num_stat(MODE, cpc_number_list, stat_dictionary)    \n",
        "    print(f\"CPC number stats in sub-group level: {result}\\n\")        "
      ],
      "metadata": {
        "id": "C9HGl0CgIoap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# truncate date information"
      ],
      "metadata": {
        "id": "T6wxqqlZuOCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the statistics of same CPC number"
      ],
      "metadata": {
        "id": "xMHNCoiOnhZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "suvk9or7m81k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "T5N5g-uqydln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dEDA5X3kyGBL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "BertFeatureVectorToCPCNumStat.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOww7sXfhLr6tN1GlCdURpv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}